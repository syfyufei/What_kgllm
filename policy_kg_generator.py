#!/usr/bin/env python3
"""
é¦™æ¸¯æ–½æ”¿æŠ¥å‘ŠçŸ¥è¯†å›¾è°±æ‰¹é‡ç”Ÿæˆå™¨
ç¬¬ä¸€é˜¶æ®µï¼šä¸ºæ¯å¹´æ–½æ”¿æŠ¥å‘Šç”Ÿæˆç‹¬ç«‹çš„çŸ¥è¯†å›¾è°±æ•°æ®
"""

import os
import json
import sys
import time
from datetime import datetime
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from src.knowledge_graph.main import process_text_in_chunks
from src.knowledge_graph.config import load_config

class PolicyKGGenerator:
    """æ–½æ”¿æŠ¥å‘ŠçŸ¥è¯†å›¾è°±ç”Ÿæˆå™¨"""

    def __init__(self, data_dir="policy_data"):
        self.data_dir = Path(data_dir)
        self.years = range(1997, 2025)  # 1997-2024å¹´
        self.setup_directories()

    def setup_directories(self):
        """è®¾ç½®ç›®å½•ç»“æ„"""
        directories = [
            self.data_dir / "raw_texts",      # åŸå§‹æ–‡æœ¬
            self.data_dir / "kg_json",        # çŸ¥è¯†å›¾è°±JSONæ•°æ®
            self.data_dir / "kg_html",        # çŸ¥è¯†å›¾è°±å¯è§†åŒ–
            self.data_dir / "logs",           # å¤„ç†æ—¥å¿—
            self.data_dir / "metadata"        # å…ƒæ•°æ®ä¿¡æ¯
        ]

        for dir_path in directories:
            dir_path.mkdir(parents=True, exist_ok=True)

        print("ğŸ“ ç›®å½•ç»“æ„å·²åˆ›å»º:")
        for dir_path in directories:
            print(f"   â€¢ {dir_path}/")

    def create_policy_config(self):
        """åˆ›å»ºé’ˆå¯¹æ–½æ”¿æŠ¥å‘Šçš„ä¸“ç”¨é…ç½®"""
        base_config = load_config()

        # é’ˆå¯¹æ–½æ”¿æŠ¥å‘Šä¼˜åŒ–çš„é…ç½®
        policy_config = base_config.copy()
        policy_config.update({
            'chunking': {
                'chunk_size': 150,  # æ–½æ”¿æŠ¥å‘Šæ®µè½è¾ƒé•¿
                'overlap': 30       # å¢åŠ é‡å ç¡®ä¿æ”¿ç­–è¿è´¯æ€§
            },
            'standardization': {
                'enabled': True,
                'use_llm_for_entities': True,
                'focus_entities': [
                    'è¡Œæ”¿é•¿å®˜', 'ç‰¹åŒºæ”¿åºœ', 'ä¸­å¤®æ”¿åºœ', 'ç«‹æ³•ä¼š',
                    'ä¸€å›½ä¸¤åˆ¶', 'åŸºæœ¬æ³•', 'å›½å®¶å®‰å…¨',
                    'ç»æµå‘å±•', 'æ°‘ç”Ÿæ”¹å–„', 'æ•™è‚²æ”¿ç­–', 'æˆ¿å±‹æ”¿ç­–',
                    'å¤§æ¹¾åŒº', 'åˆ›æ–°ç§‘æŠ€', 'é’å¹´å‘å±•'
                ]
            },
            'inference': {
                'enabled': True,
                'use_llm_for_inference': True,
                'apply_transitive': True
            }
        })

        return policy_config

    def generate_single_kg(self, year, text_content, config):
        """ä¸ºå•å¹´æ–½æ”¿æŠ¥å‘Šç”ŸæˆçŸ¥è¯†å›¾è°±"""
        print(f"\nğŸ“„ å¤„ç† {year} å¹´æ–½æ”¿æŠ¥å‘Š...")

        try:
            # å¤„ç†æ–‡æœ¬ç”ŸæˆçŸ¥è¯†å›¾è°±
            kg_data = process_text_in_chunks(config, text_content, debug=False)

            if not kg_data:
                print(f"âŒ {year}å¹´çŸ¥è¯†å›¾è°±ç”Ÿæˆå¤±è´¥")
                return None

            # æ·»åŠ å…ƒæ•°æ®
            metadata = {
                'year': year,
                'generated_at': datetime.now().isoformat(),
                'total_triples': len(kg_data),
                'unique_entities': len(self._get_unique_entities(kg_data)),
                'unique_relations': len(set(item['predicate'] for item in kg_data)),
                'text_length': len(text_content),
                'chunks_processed': max([item.get('chunk', 1) for item in kg_data])
            }

            # ä¿å­˜JSONæ•°æ®
            json_file = self.data_dir / "kg_json" / f"policy_kg_{year}.json"
            output_data = {
                'metadata': metadata,
                'knowledge_graph': kg_data
            }

            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, ensure_ascii=False, indent=2)

            print(f"âœ… {year}å¹´å¤„ç†å®Œæˆ:")
            print(f"   â€¢ ä¸‰å…ƒç»„æ•°é‡: {metadata['total_triples']}")
            print(f"   â€¢ å”¯ä¸€å®ä½“: {metadata['unique_entities']}")
            print(f"   â€¢ å…³ç³»ç±»å‹: {metadata['unique_relations']}")
            print(f"   â€¢ æ•°æ®ä¿å­˜: {json_file}")

            return metadata

        except Exception as e:
            print(f"âŒ {year}å¹´å¤„ç†å‡ºé”™: {str(e)}")
            return None

    def _get_unique_entities(self, kg_data):
        """è·å–å”¯ä¸€å®ä½“é›†åˆ"""
        entities = set()
        for item in kg_data:
            entities.add(item['subject'])
            entities.add(item['object'])
        return entities

    def batch_generate(self):
        """æ‰¹é‡ç”Ÿæˆæ‰€æœ‰å¹´ä»½çš„çŸ¥è¯†å›¾è°±"""
        print("ğŸš€ å¼€å§‹æ‰¹é‡ç”Ÿæˆé¦™æ¸¯æ–½æ”¿æŠ¥å‘ŠçŸ¥è¯†å›¾è°±")
        print("=" * 60)

        config = self.create_policy_config()
        results = {}

        # æ£€æŸ¥å¯ç”¨çš„æ–‡æœ¬æ–‡ä»¶
        available_files = []
        for year in self.years:
            text_file = self.data_dir / "raw_texts" / f"policy_address_{year}.txt"
            if text_file.exists():
                available_files.append((year, text_file))
            else:
                print(f"âš ï¸  {year}å¹´æ–‡ä»¶ä¸å­˜åœ¨: {text_file}")

        if not available_files:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•æ–½æ”¿æŠ¥å‘Šæ–‡æœ¬æ–‡ä»¶")
            print("è¯·å°†æ–‡æœ¬æ–‡ä»¶æ”¾ç½®åœ¨: policy_data/raw_texts/")
            print("æ–‡ä»¶å‘½åæ ¼å¼: policy_address_YYYY.txt")
            return

        print(f"\nğŸ“Š æ‰¾åˆ° {len(available_files)} ä¸ªæ–‡ä»¶ï¼Œå¼€å§‹å¤„ç†...")

        # å¤„ç†æ¯ä¸ªæ–‡ä»¶
        for i, (year, text_file) in enumerate(available_files, 1):
            print(f"\n[{i}/{len(available_files)}] å¤„ç† {year} å¹´...")

            try:
                # è¯»å–æ–‡æœ¬
                with open(text_file, 'r', encoding='utf-8') as f:
                    text_content = f.read()

                # ç”ŸæˆçŸ¥è¯†å›¾è°±
                metadata = self.generate_single_kg(year, text_content, config)
                if metadata:
                    results[year] = metadata

                # æ·»åŠ å»¶è¿Ÿé¿å…APIé™åˆ¶
                if i < len(available_files):
                    print("â³ ç­‰å¾…5ç§’é¿å…APIé™åˆ¶...")
                    time.sleep(5)

            except Exception as e:
                print(f"âŒ è¯»å–{year}å¹´æ–‡ä»¶å¤±è´¥: {str(e)}")
                continue

        # ä¿å­˜æ‰¹é‡å¤„ç†ç»“æœ
        self._save_batch_results(results)

        print(f"\nğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆ!")
        print(f"âœ… æˆåŠŸå¤„ç†: {len(results)} ä¸ªå¹´ä»½")
        print(f"ğŸ“ æ•°æ®ä¿å­˜åœ¨: {self.data_dir}/kg_json/")

        return results

    def _save_batch_results(self, results):
        """ä¿å­˜æ‰¹é‡å¤„ç†ç»“æœæ‘˜è¦"""
        summary = {
            'generated_at': datetime.now().isoformat(),
            'total_years': len(results),
            'years_processed': sorted(results.keys()),
            'summary_stats': {
                'total_triples': sum(r['total_triples'] for r in results.values()),
                'avg_triples_per_year': sum(r['total_triples'] for r in results.values()) / len(results) if results else 0,
                'total_entities': sum(r['unique_entities'] for r in results.values()),
                'avg_entities_per_year': sum(r['unique_entities'] for r in results.values()) / len(results) if results else 0
            },
            'yearly_details': results
        }

        summary_file = self.data_dir / "metadata" / "batch_generation_summary.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)

        print(f"ğŸ“‹ æ‰¹é‡å¤„ç†æ‘˜è¦å·²ä¿å­˜: {summary_file}")

    def check_data_status(self):
        """æ£€æŸ¥æ•°æ®ç”ŸæˆçŠ¶æ€"""
        print("ğŸ” æ£€æŸ¥çŸ¥è¯†å›¾è°±æ•°æ®çŠ¶æ€...")
        print("=" * 50)

        kg_dir = self.data_dir / "kg_json"
        existing_files = list(kg_dir.glob("policy_kg_*.json"))

        if not existing_files:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•çŸ¥è¯†å›¾è°±æ•°æ®æ–‡ä»¶")
            return {}

        data_status = {}
        for file_path in sorted(existing_files):
            try:
                year = int(file_path.stem.split('_')[-1])
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                metadata = data.get('metadata', {})
                data_status[year] = {
                    'file': file_path,
                    'triples': metadata.get('total_triples', 0),
                    'entities': metadata.get('unique_entities', 0),
                    'relations': metadata.get('unique_relations', 0),
                    'generated_at': metadata.get('generated_at', 'Unknown')
                }

                print(f"âœ… {year}å¹´: {metadata.get('total_triples', 0)} ä¸‰å…ƒç»„, "
                      f"{metadata.get('unique_entities', 0)} å®ä½“, "
                      f"{metadata.get('unique_relations', 0)} å…³ç³»")

            except Exception as e:
                print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {str(e)}")

        print(f"\nğŸ“Š æ€»è®¡: {len(data_status)} ä¸ªå¹´ä»½çš„æ•°æ®å·²ç”Ÿæˆ")
        return data_status

def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description='é¦™æ¸¯æ–½æ”¿æŠ¥å‘ŠçŸ¥è¯†å›¾è°±æ‰¹é‡ç”Ÿæˆå™¨')
    parser.add_argument('--generate', action='store_true', help='æ‰¹é‡ç”ŸæˆçŸ¥è¯†å›¾è°±')
    parser.add_argument('--check', action='store_true', help='æ£€æŸ¥æ•°æ®çŠ¶æ€')
    parser.add_argument('--data-dir', default='policy_data', help='æ•°æ®ç›®å½•è·¯å¾„')

    args = parser.parse_args()

    generator = PolicyKGGenerator(args.data_dir)

    if args.generate:
        generator.batch_generate()
    elif args.check:
        generator.check_data_status()
    else:
        print("è¯·æŒ‡å®šæ“ä½œ:")
        print("  --generate  æ‰¹é‡ç”ŸæˆçŸ¥è¯†å›¾è°±")
        print("  --check     æ£€æŸ¥æ•°æ®çŠ¶æ€")

if __name__ == "__main__":
    main()
