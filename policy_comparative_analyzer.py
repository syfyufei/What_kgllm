#!/usr/bin/env python3
"""
é¦™æ¸¯æ–½æ”¿æŠ¥å‘ŠçŸ¥è¯†å›¾è°±å¯¹æ¯”åˆ†æå™¨
ç¬¬äºŒé˜¶æ®µï¼šåŸºäºå·²ç”Ÿæˆçš„çŸ¥è¯†å›¾è°±æ•°æ®è¿›è¡Œå¤šç»´åº¦å¯¹æ¯”åˆ†æ
"""

import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import networkx as nx
from pathlib import Path
from collections import defaultdict, Counter
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

class PolicyComparativeAnalyzer:
    """æ–½æ”¿æŠ¥å‘Šå¯¹æ¯”åˆ†æå™¨"""

    def __init__(self, data_dir="policy_data"):
        self.data_dir = Path(data_dir)
        self.kg_data = {}  # å­˜å‚¨åŠ è½½çš„çŸ¥è¯†å›¾è°±æ•°æ®
        self.analysis_results = {}  # å­˜å‚¨åˆ†æç»“æœ

        # è®¾ç½®ä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False

    def load_kg_data(self):
        """åŠ è½½æ‰€æœ‰å¹´ä»½çš„çŸ¥è¯†å›¾è°±æ•°æ®"""
        print("ğŸ“‚ åŠ è½½çŸ¥è¯†å›¾è°±æ•°æ®...")

        kg_dir = self.data_dir / "kg_json"
        if not kg_dir.exists():
            print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {kg_dir}")
            return False

        json_files = list(kg_dir.glob("policy_kg_*.json"))
        if not json_files:
            print(f"âŒ æœªæ‰¾åˆ°çŸ¥è¯†å›¾è°±æ•°æ®æ–‡ä»¶")
            return False

        for file_path in sorted(json_files):
            try:
                year = int(file_path.stem.split('_')[-1])
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                self.kg_data[year] = {
                    'metadata': data.get('metadata', {}),
                    'triples': data.get('knowledge_graph', [])
                }

                print(f"âœ… {year}å¹´: {len(self.kg_data[year]['triples'])} ä¸ªä¸‰å…ƒç»„")

            except Exception as e:
                print(f"âŒ åŠ è½½å¤±è´¥ {file_path}: {str(e)}")

        print(f"ğŸ“Š æ€»è®¡åŠ è½½: {len(self.kg_data)} ä¸ªå¹´ä»½çš„æ•°æ®")
        return len(self.kg_data) > 0

    def analyze_theme_evolution(self):
        """åˆ†æè¯è¯­ä¸»é¢˜æ¼”å˜"""
        print("\n" + "="*60)
        print("ğŸ“ˆ è¯è¯­ä¸»é¢˜æ¼”å˜åˆ†æ")
        print("="*60)

        # å®šä¹‰è¯è¯­ä¸»é¢˜å…³é”®è¯
        theme_keywords = {
            'ä¸€å›½ä¸¤åˆ¶': ['ä¸€å›½ä¸¤åˆ¶', 'åŸºæœ¬æ³•', 'é«˜åº¦è‡ªæ²»', 'æ¸¯äººæ²»æ¸¯', 'ä¸­å¤®æ”¿åºœ'],
            'ç»æµå‘å±•': ['ç»æµ', 'å‘å±•', 'æŠ•èµ„', 'é‡‘è', 'è´¸æ˜“', 'å¸‚åœº', 'äº§ä¸š'],
            'æ°‘ç”Ÿç¦åˆ©': ['æ°‘ç”Ÿ', 'ç¦åˆ©', 'æ•™è‚²', 'åŒ»ç–—', 'ä½æˆ¿', 'å°±ä¸š', 'ç¤¾ä¼šä¿éšœ'],
            'å›½å®¶å®‰å…¨': ['å›½å®¶å®‰å…¨', 'å®‰å…¨', 'ç¨³å®š', 'ç»´æŠ¤', 'æ³•å¾‹', 'ç§©åº'],
            'åˆ›æ–°ç§‘æŠ€': ['åˆ›æ–°', 'ç§‘æŠ€', 'æŠ€æœ¯', 'æ•°å­—', 'æ™ºèƒ½', 'ç ”å‘', 'äººå·¥æ™ºèƒ½'],
            'å›½é™…åˆä½œ': ['å›½é™…', 'åˆä½œ', 'å…¨çƒ', 'ä¸–ç•Œ', 'å¤–å›½', 'å¼€æ”¾', 'äº¤æµ'],
            'ç²¤æ¸¯æ¾³å¤§æ¹¾åŒº': ['å¤§æ¹¾åŒº', 'ç²¤æ¸¯æ¾³', 'æ·±åœ³', 'å¹¿å·', 'ç æµ·', 'èåˆ'],
            'é’å¹´å‘å±•': ['é’å¹´', 'å¹´è½»äºº', 'å­¦ç”Ÿ', 'åŸ¹è®­', 'æœºä¼š', 'å°±ä¸š'],
            'æ–‡åŒ–å»ºè®¾': ['æ–‡åŒ–', 'è‰ºæœ¯', 'ä½“è‚²', 'æ—…æ¸¸', 'ä¼ ç»Ÿ', 'é—äº§', 'åˆ›æ„'],
            'ç¯å¢ƒä¿æŠ¤': ['ç¯å¢ƒ', 'ç¯ä¿', 'ç»¿è‰²', 'å¯æŒç»­', 'æ°”å€™', 'ç”Ÿæ€', 'èŠ‚èƒ½']
        }

        theme_evolution = defaultdict(dict)

        for year, data in self.kg_data.items():
            triples = data['triples']
            total_triples = len(triples)

            for theme, keywords in theme_keywords.items():
                count = 0
                for triple in triples:
                    # æ£€æŸ¥ä¸‰å…ƒç»„ä¸­æ˜¯å¦åŒ…å«ä¸»é¢˜å…³é”®è¯
                    text = f"{triple['subject']} {triple['predicate']} {triple['object']}"
                    if any(keyword in text for keyword in keywords):
                        count += 1

                percentage = (count / total_triples * 100) if total_triples > 0 else 0
                theme_evolution[theme][year] = {
                    'count': count,
                    'percentage': percentage,
                    'total_triples': total_triples
                }

        self.analysis_results['theme_evolution'] = theme_evolution
        return theme_evolution

    def analyze_entity_networks(self):
        """åˆ†æå®ä½“ç½‘ç»œç»“æ„"""
        print("\n" + "="*50)
        print("ğŸ•¸ï¸ å®ä½“ç½‘ç»œç»“æ„åˆ†æ")
        print("="*50)

        network_metrics = {}

        for year, data in self.kg_data.items():
            triples = data['triples']

            # æ„å»ºç½‘ç»œå›¾
            G = nx.Graph()
            for triple in triples:
                G.add_edge(triple['subject'], triple['object'],
                          relation=triple['predicate'])

            # è®¡ç®—ç½‘ç»œæŒ‡æ ‡
            if len(G.nodes()) > 0:
                metrics = {
                    'nodes': len(G.nodes()),
                    'edges': len(G.edges()),
                    'density': nx.density(G),
                    'avg_clustering': nx.average_clustering(G),
                    'components': nx.number_connected_components(G)
                }

                # ä¸­å¿ƒæ€§åˆ†æ
                if len(G.nodes()) > 1:
                    degree_centrality = nx.degree_centrality(G)
                    betweenness_centrality = nx.betweenness_centrality(G)

                    metrics['top_degree_entities'] = sorted(
                        degree_centrality.items(),
                        key=lambda x: x[1], reverse=True
                    )[:10]

                    metrics['top_betweenness_entities'] = sorted(
                        betweenness_centrality.items(),
                        key=lambda x: x[1], reverse=True
                    )[:10]

                network_metrics[year] = metrics

                print(f"{year}å¹´: {metrics['nodes']}èŠ‚ç‚¹, {metrics['edges']}è¾¹, "
                      f"å¯†åº¦{metrics['density']:.3f}, èšç±»{metrics['avg_clustering']:.3f}")

        self.analysis_results['network_metrics'] = network_metrics
        return network_metrics

    def analyze_relationship_patterns(self):
        """åˆ†æå…³ç³»æ¨¡å¼å˜åŒ–"""
        print("\n" + "="*50)
        print("ğŸ”— å…³ç³»æ¨¡å¼å˜åŒ–åˆ†æ")
        print("="*50)

        relation_evolution = {}

        for year, data in self.kg_data.items():
            triples = data['triples']

            # ç»Ÿè®¡å…³ç³»ç±»å‹
            relations = [triple['predicate'] for triple in triples]
            relation_counts = Counter(relations)

            # è·å–å‰20ä¸ªæœ€å¸¸è§å…³ç³»
            top_relations = dict(relation_counts.most_common(20))

            relation_evolution[year] = {
                'total_relations': len(relations),
                'unique_relations': len(relation_counts),
                'top_relations': top_relations,
                'relation_diversity': len(relation_counts) / len(relations) if relations else 0
            }

            print(f"{year}å¹´: {len(relations)}ä¸ªå…³ç³», {len(relation_counts)}ç§ç±»å‹, "
                  f"å¤šæ ·æ€§{relation_evolution[year]['relation_diversity']:.3f}")

        self.analysis_results['relation_evolution'] = relation_evolution
        return relation_evolution

    def identify_discourse_shifts(self, theme_evolution):
        """è¯†åˆ«è¯è¯­è½¬å˜ç‚¹"""
        print("\n" + "="*50)
        print("ğŸ”„ è¯è¯­è½¬å˜ç‚¹è¯†åˆ«")
        print("="*50)

        shifts = []

        for theme, yearly_data in theme_evolution.items():
            years = sorted(yearly_data.keys())
            if len(years) < 2:
                continue

            percentages = [yearly_data[year]['percentage'] for year in years]

            # è¯†åˆ«æ˜¾è‘—å˜åŒ–ç‚¹ï¼ˆå˜åŒ–è¶…è¿‡é˜ˆå€¼ï¼‰
            for i in range(1, len(percentages)):
                change = percentages[i] - percentages[i-1]
                if abs(change) > 3:  # 3%ä»¥ä¸Šçš„å˜åŒ–
                    shifts.append({
                        'year': years[i],
                        'theme': theme,
                        'change': change,
                        'from_percentage': percentages[i-1],
                        'to_percentage': percentages[i],
                        'significance': 'major' if abs(change) > 8 else 'moderate'
                    })

        # æŒ‰å¹´ä»½å’Œå˜åŒ–å¹…åº¦æ’åº
        shifts.sort(key=lambda x: (x['year'], abs(x['change'])), reverse=True)

        print("ğŸ“Š ä¸»è¦è¯è¯­è½¬å˜ç‚¹:")
        for shift in shifts[:20]:
            direction = "â†—ï¸" if shift['change'] > 0 else "â†˜ï¸"
            significance = "ğŸ”¥" if shift['significance'] == 'major' else "ğŸ“ˆ"
            print(f"{shift['year']}å¹´ {shift['theme']} {direction} {significance} "
                  f"{shift['change']:+.1f}% "
                  f"({shift['from_percentage']:.1f}% â†’ {shift['to_percentage']:.1f}%)")

        self.analysis_results['discourse_shifts'] = shifts
        return shifts

    def create_visualizations(self):
        """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
        print("\n" + "="*50)
        print("ğŸ“Š ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨")
        print("="*50)

        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        viz_dir = self.data_dir / "visualizations"
        viz_dir.mkdir(exist_ok=True)

        # 1. ä¸»é¢˜æ¼”å˜è¶‹åŠ¿å›¾
        self._create_theme_evolution_plot(viz_dir)

        # 2. ä¸»é¢˜çƒ­åŠ›å›¾
        self._create_theme_heatmap(viz_dir)

        # 3. ç½‘ç»œæŒ‡æ ‡å˜åŒ–å›¾
        self._create_network_metrics_plot(viz_dir)

        # 4. å…³ç³»å¤šæ ·æ€§å˜åŒ–å›¾
        self._create_relation_diversity_plot(viz_dir)

        print("âœ… æ‰€æœ‰å¯è§†åŒ–å›¾è¡¨å·²ç”Ÿæˆ")

    def _create_theme_evolution_plot(self, viz_dir):
        """åˆ›å»ºä¸»é¢˜æ¼”å˜è¶‹åŠ¿å›¾"""
        if 'theme_evolution' not in self.analysis_results:
            return

        theme_evolution = self.analysis_results['theme_evolution']

        # é€‰æ‹©é‡ç‚¹ä¸»é¢˜
        key_themes = ['ä¸€å›½ä¸¤åˆ¶', 'ç»æµå‘å±•', 'å›½å®¶å®‰å…¨', 'ç²¤æ¸¯æ¾³å¤§æ¹¾åŒº', 'åˆ›æ–°ç§‘æŠ€', 'æ°‘ç”Ÿç¦åˆ©']

        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('é¦™æ¸¯æ–½æ”¿æŠ¥å‘Šè¯è¯­ä¸»é¢˜æ¼”å˜è¶‹åŠ¿ (1997-2024)', fontsize=16, fontweight='bold')

        for i, theme in enumerate(key_themes):
            if theme not in theme_evolution:
                continue

            ax = axes[i//3, i%3]

            years = sorted(theme_evolution[theme].keys())
            percentages = [theme_evolution[theme][year]['percentage'] for year in years]

            ax.plot(years, percentages, marker='o', linewidth=2.5, markersize=6,
                   color=plt.cm.Set3(i), alpha=0.8)
            ax.fill_between(years, percentages, alpha=0.3, color=plt.cm.Set3(i))

            ax.set_title(f'{theme}', fontsize=12, fontweight='bold')
            ax.set_xlabel('å¹´ä»½', fontsize=10)
            ax.set_ylabel('å æ¯” (%)', fontsize=10)
            ax.grid(True, alpha=0.3)
            ax.set_ylim(0, max(percentages) * 1.1 if percentages else 1)

        plt.tight_layout()
        plt.savefig(viz_dir / 'theme_evolution_trends.png', dpi=300, bbox_inches='tight')
        plt.close()
        print("âœ… ä¸»é¢˜æ¼”å˜è¶‹åŠ¿å›¾å·²ç”Ÿæˆ")

    def _create_theme_heatmap(self, viz_dir):
        """åˆ›å»ºä¸»é¢˜çƒ­åŠ›å›¾"""
        if 'theme_evolution' not in self.analysis_results:
            return

        theme_evolution = self.analysis_results['theme_evolution']

        # æ„å»ºæ•°æ®çŸ©é˜µ
        years = sorted(list(self.kg_data.keys()))
        themes = list(theme_evolution.keys())

        data_matrix = []
        for theme in themes:
            row = []
            for year in years:
                if year in theme_evolution[theme]:
                    row.append(theme_evolution[theme][year]['percentage'])
                else:
                    row.append(0)
            data_matrix.append(row)

        # åˆ›å»ºçƒ­åŠ›å›¾
        plt.figure(figsize=(16, 10))
        sns.heatmap(data_matrix,
                   xticklabels=years,
                   yticklabels=themes,
                   annot=True,
                   fmt='.1f',
                   cmap='YlOrRd',
                   cbar_kws={'label': 'ä¸»é¢˜å æ¯” (%)'})

        plt.title('é¦™æ¸¯æ–½æ”¿æŠ¥å‘Šè¯è¯­ä¸»é¢˜çƒ­åŠ›å›¾ (1997-2024)', fontsize=14, fontweight='bold')
        plt.xlabel('å¹´ä»½', fontsize=12)
        plt.ylabel('è¯è¯­ä¸»é¢˜', fontsize=12)
        plt.xticks(rotation=45)
        plt.yticks(rotation=0)
        plt.tight_layout()
        plt.savefig(viz_dir / 'theme_heatmap.png', dpi=300, bbox_inches='tight')
        plt.close()
        print("âœ… ä¸»é¢˜çƒ­åŠ›å›¾å·²ç”Ÿæˆ")

    def _create_network_metrics_plot(self, viz_dir):
        """åˆ›å»ºç½‘ç»œæŒ‡æ ‡å˜åŒ–å›¾"""
        if 'network_metrics' not in self.analysis_results:
            return

        network_metrics = self.analysis_results['network_metrics']

        years = sorted(network_metrics.keys())
        nodes = [network_metrics[year]['nodes'] for year in years]
        edges = [network_metrics[year]['edges'] for year in years]
        density = [network_metrics[year]['density'] for year in years]
        clustering = [network_metrics[year]['avg_clustering'] for year in years]

        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('çŸ¥è¯†å›¾è°±ç½‘ç»œç»“æ„æŒ‡æ ‡å˜åŒ–', fontsize=16, fontweight='bold')

        # èŠ‚ç‚¹æ•°é‡
        axes[0,0].plot(years, nodes, marker='o', color='blue', linewidth=2)
        axes[0,0].set_title('å®ä½“æ•°é‡å˜åŒ–')
        axes[0,0].set_ylabel('èŠ‚ç‚¹æ•°')
        axes[0,0].grid(True, alpha=0.3)

        # è¾¹æ•°é‡
        axes[0,1].plot(years, edges, marker='s', color='red', linewidth=2)
        axes[0,1].set_title('å…³ç³»æ•°é‡å˜åŒ–')
        axes[0,1].set_ylabel('è¾¹æ•°')
        axes[0,1].grid(True, alpha=0.3)

        # ç½‘ç»œå¯†åº¦
        axes[1,0].plot(years, density, marker='^', color='green', linewidth=2)
        axes[1,0].set_title('ç½‘ç»œå¯†åº¦å˜åŒ–')
        axes[1,0].set_ylabel('å¯†åº¦')
        axes[1,0].set_xlabel('å¹´ä»½')
        axes[1,0].grid(True, alpha=0.3)

        # èšç±»ç³»æ•°
        axes[1,1].plot(years, clustering, marker='d', color='orange', linewidth=2)
        axes[1,1].set_title('å¹³å‡èšç±»ç³»æ•°å˜åŒ–')
        axes[1,1].set_ylabel('èšç±»ç³»æ•°')
        axes[1,1].set_xlabel('å¹´ä»½')
        axes[1,1].grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig(viz_dir / 'network_metrics.png', dpi=300, bbox_inches='tight')
        plt.close()
        print("âœ… ç½‘ç»œæŒ‡æ ‡å›¾å·²ç”Ÿæˆ")

    def _create_relation_diversity_plot(self, viz_dir):
        """åˆ›å»ºå…³ç³»å¤šæ ·æ€§å˜åŒ–å›¾"""
        if 'relation_evolution' not in self.analysis_results:
            return

        relation_evolution = self.analysis_results['relation_evolution']

        years = sorted(relation_evolution.keys())
        diversity = [relation_evolution[year]['relation_diversity'] for year in years]
        unique_relations = [relation_evolution[year]['unique_relations'] for year in years]

        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        fig.suptitle('å…³ç³»æ¨¡å¼å¤šæ ·æ€§åˆ†æ', fontsize=16, fontweight='bold')

        # å…³ç³»å¤šæ ·æ€§
        ax1.plot(years, diversity, marker='o', color='purple', linewidth=2.5)
        ax1.set_title('å…³ç³»å¤šæ ·æ€§å˜åŒ–')
        ax1.set_xlabel('å¹´ä»½')
        ax1.set_ylabel('å¤šæ ·æ€§æŒ‡æ•°')
        ax1.grid(True, alpha=0.3)

        # å”¯ä¸€å…³ç³»ç±»å‹æ•°é‡
        ax2.bar(years, unique_relations, color='skyblue', alpha=0.7)
        ax2.set_title('å”¯ä¸€å…³ç³»ç±»å‹æ•°é‡')
        ax2.set_xlabel('å¹´ä»½')
        ax2.set_ylabel('å…³ç³»ç±»å‹æ•°')
        ax2.grid(True, alpha=0.3, axis='y')

        plt.tight_layout()
        plt.savefig(viz_dir / 'relation_diversity.png', dpi=300, bbox_inches='tight')
        plt.close()
        print("âœ… å…³ç³»å¤šæ ·æ€§å›¾å·²ç”Ÿæˆ")

    def generate_analysis_report(self):
        """ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"""
        print("\n" + "="*60)
        print("ğŸ“‹ ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š")
        print("="*60)

        report_content = self._create_report_content()

        # ä¿å­˜æŠ¥å‘Š
        report_dir = self.data_dir / "analysis"
        report_dir.mkdir(exist_ok=True)

        report_file = report_dir / f"comparative_analysis_report_{datetime.now().strftime('%Y%m%d')}.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)

        print(f"ğŸ“„ åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        return report_file

    def _create_report_content(self):
        """åˆ›å»ºæŠ¥å‘Šå†…å®¹"""
        years_range = f"{min(self.kg_data.keys())}-{max(self.kg_data.keys())}"
        total_years = len(self.kg_data)

        report = f"""# é¦™æ¸¯æ–½æ”¿æŠ¥å‘ŠçŸ¥è¯†å›¾è°±å¯¹æ¯”åˆ†ææŠ¥å‘Š

## ğŸ“Š æ•°æ®æ¦‚è§ˆ
- **åˆ†ææ—¶æœŸ**: {years_range}
- **æŠ¥å‘Šæ•°é‡**: {total_years} ä»½
- **ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')}

## ğŸ¯ ä¸»è¦å‘ç°

### 1. è¯è¯­ä¸»é¢˜æ¼”å˜ç‰¹å¾
"""

        # æ·»åŠ ä¸»é¢˜æ¼”å˜åˆ†æ
        if 'theme_evolution' in self.analysis_results:
            theme_evolution = self.analysis_results['theme_evolution']

            for theme in ['ä¸€å›½ä¸¤åˆ¶', 'ç»æµå‘å±•', 'å›½å®¶å®‰å…¨', 'ç²¤æ¸¯æ¾³å¤§æ¹¾åŒº']:
                if theme in theme_evolution:
                    years = sorted(theme_evolution[theme].keys())
                    if len(years) >= 2:
                        start_pct = theme_evolution[theme][years[0]]['percentage']
                        end_pct = theme_evolution[theme][years[-1]]['percentage']
                        trend = "ä¸Šå‡" if end_pct > start_pct else "ä¸‹é™"
                        change = abs(end_pct - start_pct)

                        report += f"\n- **{theme}**: {trend}è¶‹åŠ¿ï¼Œå˜åŒ–å¹…åº¦ {change:.1f}%"

        report += f"""

### 2. ç½‘ç»œç»“æ„æ¼”å˜
"""

        # æ·»åŠ ç½‘ç»œåˆ†æ
        if 'network_metrics' in self.analysis_results:
            network_metrics = self.analysis_results['network_metrics']
            years = sorted(network_metrics.keys())

            if years:
                start_year = years[0]
                end_year = years[-1]

                start_nodes = network_metrics[start_year]['nodes']
                end_nodes = network_metrics[end_year]['nodes']

                start_density = network_metrics[start_year]['density']
                end_density = network_metrics[end_year]['density']

                report += f"""
- **å®ä½“è§„æ¨¡**: ä»{start_year}å¹´çš„{start_nodes}ä¸ªå®ä½“å¢é•¿åˆ°{end_year}å¹´çš„{end_nodes}ä¸ªå®ä½“
- **ç½‘ç»œå¯†åº¦**: ä»{start_density:.3f}å˜åŒ–åˆ°{end_density:.3f}
"""

        report += f"""

### 3. å…³é”®è½¬å˜èŠ‚ç‚¹
"""

        # æ·»åŠ è½¬å˜ç‚¹åˆ†æ
        if 'discourse_shifts' in self.analysis_results:
            shifts = self.analysis_results['discourse_shifts']
            major_shifts = [s for s in shifts if s['significance'] == 'major'][:10]

            for shift in major_shifts:
                direction = "æ˜¾è‘—ä¸Šå‡" if shift['change'] > 0 else "æ˜¾è‘—ä¸‹é™"
                report += f"\n- **{shift['year']}å¹´**: {shift['theme']}è¯è¯­{direction} ({shift['change']:+.1f}%)"

        report += f"""

## ğŸ“ˆ åˆ†ææ–¹æ³•
1. **çŸ¥è¯†å›¾è°±æ„å»º**: ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ä»æ–‡æœ¬ä¸­æå–å®ä½“å…³ç³»ä¸‰å…ƒç»„
2. **ä¸»é¢˜åˆ†ç±»**: åŸºäºå…³é”®è¯åŒ¹é…è¯†åˆ«10ä¸ªä¸»è¦è¯è¯­ä¸»é¢˜
3. **ç½‘ç»œåˆ†æ**: è®¡ç®—å›¾ç»“æ„æŒ‡æ ‡ï¼ˆå¯†åº¦ã€èšç±»ç³»æ•°ã€ä¸­å¿ƒæ€§ç­‰ï¼‰
4. **æ—¶åºå¯¹æ¯”**: è¯†åˆ«è¯è¯­é‡ç‚¹çš„æ˜¾è‘—å˜åŒ–èŠ‚ç‚¹

## ğŸ“Š æ•°æ®æ–‡ä»¶
- åŸå§‹æ•°æ®: `kg_json/policy_kg_YYYY.json`
- å¯è§†åŒ–å›¾è¡¨: `visualizations/`
- è¯¦ç»†åˆ†æ: `analysis/`

---
*æŠ¥å‘Šç”±çŸ¥è¯†å›¾è°±å¯¹æ¯”åˆ†æç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆ*
"""

        return report

    def run_full_analysis(self):
        """è¿è¡Œå®Œæ•´çš„å¯¹æ¯”åˆ†ææµç¨‹"""
        print("ğŸš€ å¼€å§‹é¦™æ¸¯æ–½æ”¿æŠ¥å‘ŠçŸ¥è¯†å›¾è°±å¯¹æ¯”åˆ†æ")
        print("="*60)

        # 1. åŠ è½½æ•°æ®
        if not self.load_kg_data():
            print("âŒ æ•°æ®åŠ è½½å¤±è´¥ï¼Œè¯·å…ˆç”ŸæˆçŸ¥è¯†å›¾è°±æ•°æ®")
            return False

        # 2. æ‰§è¡Œå„é¡¹åˆ†æ
        print("\nğŸ” æ‰§è¡Œå¤šç»´åº¦åˆ†æ...")
        theme_evolution = self.analyze_theme_evolution()
        network_metrics = self.analyze_entity_networks()
        relation_patterns = self.analyze_relationship_patterns()

        # 3. è¯†åˆ«è½¬å˜ç‚¹
        discourse_shifts = self.identify_discourse_shifts(theme_evolution)

        # 4. åˆ›å»ºå¯è§†åŒ–
        self.create_visualizations()

        # 5. ç”ŸæˆæŠ¥å‘Š
        report_file = self.generate_analysis_report()

        print(f"\nğŸ‰ å¯¹æ¯”åˆ†æå®Œæˆ!")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {self.data_dir}/")
        print(f"ğŸ“„ åˆ†ææŠ¥å‘Š: {report_file}")

        return True

def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description='é¦™æ¸¯æ–½æ”¿æŠ¥å‘ŠçŸ¥è¯†å›¾è°±å¯¹æ¯”åˆ†æå™¨')
    parser.add_argument('--analyze', action='store_true', help='æ‰§è¡Œå®Œæ•´å¯¹æ¯”åˆ†æ')
    parser.add_argument('--data-dir', default='policy_data', help='æ•°æ®ç›®å½•è·¯å¾„')

    args = parser.parse_args()

    analyzer = PolicyComparativeAnalyzer(args.data_dir)

    if args.analyze:
        analyzer.run_full_analysis()
    else:
        print("è¯·æŒ‡å®šæ“ä½œ:")
        print("  --analyze   æ‰§è¡Œå®Œæ•´å¯¹æ¯”åˆ†æ")

if __name__ == "__main__":
    main()
