#!/usr/bin/env python3
"""åˆ†æçŸ¥è¯†å›¾è°±JSONæ•°æ®çš„è„šæœ¬"""

import json
import pandas as pd
from collections import Counter, defaultdict
import networkx as nx
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
import jieba
import re

def load_kg_data(file_path):
    """åŠ è½½çŸ¥è¯†å›¾è°±æ•°æ®"""
    print(f"ğŸ“‚ åŠ è½½çŸ¥è¯†å›¾è°±æ•°æ®: {file_path}")
    
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"âœ… æˆåŠŸåŠ è½½ {len(data)} ä¸ªä¸‰å…ƒç»„")
    return data

def basic_statistics(data):
    """åŸºç¡€ç»Ÿè®¡åˆ†æ"""
    print("\n" + "="*50)
    print("ğŸ“Š åŸºç¡€ç»Ÿè®¡åˆ†æ")
    print("="*50)
    
    # åŸºæœ¬æ•°é‡ç»Ÿè®¡
    total_triples = len(data)
    subjects = set(item['subject'] for item in data)
    predicates = set(item['predicate'] for item in data)
    objects = set(item['object'] for item in data)
    
    print(f"æ€»ä¸‰å…ƒç»„æ•°é‡: {total_triples:,}")
    print(f"å”¯ä¸€ä¸»ä½“æ•°é‡: {len(subjects):,}")
    print(f"å”¯ä¸€å…³ç³»æ•°é‡: {len(predicates):,}")
    print(f"å”¯ä¸€å®¢ä½“æ•°é‡: {len(objects):,}")
    print(f"æ€»å®ä½“æ•°é‡: {len(subjects | objects):,}")
    
    return {
        'total_triples': total_triples,
        'unique_subjects': len(subjects),
        'unique_predicates': len(predicates),
        'unique_objects': len(objects),
        'total_entities': len(subjects | objects)
    }

def analyze_predicates(data):
    """åˆ†æå…³ç³»ç±»å‹"""
    print("\n" + "="*50)
    print("ğŸ”— å…³ç³»ç±»å‹åˆ†æ")
    print("="*50)
    
    predicates = [item['predicate'] for item in data]
    predicate_counts = Counter(predicates)
    
    print("ğŸ“ˆ å‰20ä¸ªæœ€å¸¸è§çš„å…³ç³»ç±»å‹:")
    for i, (pred, count) in enumerate(predicate_counts.most_common(20), 1):
        percentage = (count / len(data)) * 100
        print(f"{i:2d}. {pred:<20} {count:>6,} ({percentage:5.1f}%)")
    
    return predicate_counts

def analyze_entities(data):
    """åˆ†æå®ä½“"""
    print("\n" + "="*50)
    print("ğŸ›ï¸ å®ä½“åˆ†æ")
    print("="*50)
    
    # ç»Ÿè®¡å®ä½“å‡ºç°é¢‘ç‡ï¼ˆä½œä¸ºä¸»ä½“å’Œå®¢ä½“ï¼‰
    entity_counts = defaultdict(int)
    
    for item in data:
        entity_counts[item['subject']] += 1
        entity_counts[item['object']] += 1
    
    print("ğŸ“ˆ å‰20ä¸ªæœ€é‡è¦çš„å®ä½“ï¼ˆæŒ‰å‡ºç°é¢‘ç‡ï¼‰:")
    sorted_entities = sorted(entity_counts.items(), key=lambda x: x[1], reverse=True)
    
    for i, (entity, count) in enumerate(sorted_entities[:20], 1):
        print(f"{i:2d}. {entity:<30} {count:>4} æ¬¡")
    
    return entity_counts

def analyze_subjects_objects(data):
    """åˆ†æä¸»ä½“å’Œå®¢ä½“çš„åˆ†å¸ƒ"""
    print("\n" + "="*50)
    print("ğŸ‘¥ ä¸»ä½“å’Œå®¢ä½“åˆ†æ")
    print("="*50)
    
    subjects = [item['subject'] for item in data]
    objects = [item['object'] for item in data]
    
    subject_counts = Counter(subjects)
    object_counts = Counter(objects)
    
    print("ğŸ“Š æœ€æ´»è·ƒçš„ä¸»ä½“ï¼ˆå‘èµ·æœ€å¤šå…³ç³»ï¼‰:")
    for i, (subj, count) in enumerate(subject_counts.most_common(10), 1):
        print(f"{i:2d}. {subj:<30} {count:>4} ä¸ªå…³ç³»")
    
    print("\nğŸ“Š æœ€å—å…³æ³¨çš„å®¢ä½“ï¼ˆè¢«æåŠæœ€å¤šï¼‰:")
    for i, (obj, count) in enumerate(object_counts.most_common(10), 1):
        print(f"{i:2d}. {obj:<30} {count:>4} æ¬¡è¢«æåŠ")
    
    return subject_counts, object_counts

def analyze_chunks(data):
    """åˆ†ææ–‡æ¡£å—åˆ†å¸ƒ"""
    print("\n" + "="*50)
    print("ğŸ“„ æ–‡æ¡£å—åˆ†æ")
    print("="*50)
    
    # ç»Ÿè®¡æ¯ä¸ªchunkçš„ä¸‰å…ƒç»„æ•°é‡
    chunk_counts = defaultdict(int)
    for item in data:
        if 'chunk' in item:
            chunk_counts[item['chunk']] += 1
    
    if chunk_counts:
        chunks = list(chunk_counts.values())
        print(f"æ€»æ–‡æ¡£å—æ•°: {len(chunk_counts)}")
        print(f"å¹³å‡æ¯å—ä¸‰å…ƒç»„æ•°: {sum(chunks)/len(chunks):.1f}")
        print(f"æœ€å¤šä¸‰å…ƒç»„çš„å—: {max(chunks)} ä¸ª")
        print(f"æœ€å°‘ä¸‰å…ƒç»„çš„å—: {min(chunks)} ä¸ª")
        
        # æ‰¾å‡ºæœ€ä¸°å¯Œçš„å—
        top_chunks = sorted(chunk_counts.items(), key=lambda x: x[1], reverse=True)[:5]
        print("\nğŸ“Š ä¿¡æ¯æœ€ä¸°å¯Œçš„5ä¸ªæ–‡æ¡£å—:")
        for chunk_id, count in top_chunks:
            print(f"å— {chunk_id}: {count} ä¸ªä¸‰å…ƒç»„")
    
    return chunk_counts

def find_key_topics(data):
    """è¯†åˆ«å…³é”®ä¸»é¢˜"""
    print("\n" + "="*50)
    print("ğŸ¯ å…³é”®ä¸»é¢˜è¯†åˆ«")
    print("="*50)
    
    # åŸºäºå®ä½“å’Œå…³ç³»è¯†åˆ«ä¸»é¢˜
    government_terms = ['æ”¿åºœ', 'è¡Œæ”¿é•¿å®˜', 'ç«‹æ³•ä¼š', 'å¸é•¿', 'å±€é•¿', 'éƒ¨é—¨']
    economy_terms = ['ç»æµ', 'å‘å±•', 'æŠ•èµ„', 'äº§ä¸š', 'é‡‘è', 'è´¸æ˜“']
    society_terms = ['æ°‘ç”Ÿ', 'å¸‚æ°‘', 'ç¤¾ä¼š', 'æ•™è‚²', 'åŒ»ç–—', 'ä½æˆ¿']
    policy_terms = ['æ”¿ç­–', 'æªæ–½', 'è®¡åˆ’', 'æ–¹æ¡ˆ', 'æ”¹é©', 'å»ºè®¾']
    
    topics = {
        'æ”¿åºœæ²»ç†': government_terms,
        'ç»æµå‘å±•': economy_terms,
        'ç¤¾ä¼šæ°‘ç”Ÿ': society_terms,
        'æ”¿ç­–æªæ–½': policy_terms
    }
    
    topic_counts = defaultdict(int)
    
    for item in data:
        text = f"{item['subject']} {item['predicate']} {item['object']}"
        for topic, terms in topics.items():
            if any(term in text for term in terms):
                topic_counts[topic] += 1
    
    print("ğŸ“Š ä¸»é¢˜åˆ†å¸ƒ:")
    total = sum(topic_counts.values())
    for topic, count in sorted(topic_counts.items(), key=lambda x: x[1], reverse=True):
        percentage = (count / total) * 100 if total > 0 else 0
        print(f"{topic:<12} {count:>6,} ({percentage:5.1f}%)")
    
    return topic_counts

def analyze_relationship_patterns(data):
    """åˆ†æå…³ç³»æ¨¡å¼"""
    print("\n" + "="*50)
    print("ğŸ”„ å…³ç³»æ¨¡å¼åˆ†æ")
    print("="*50)
    
    # åˆ†æä¸»ä½“-å…³ç³»ç»„åˆ
    subj_pred_pairs = [(item['subject'], item['predicate']) for item in data]
    subj_pred_counts = Counter(subj_pred_pairs)
    
    print("ğŸ“Š æœ€å¸¸è§çš„ä¸»ä½“-å…³ç³»ç»„åˆ:")
    for i, ((subj, pred), count) in enumerate(subj_pred_counts.most_common(10), 1):
        print(f"{i:2d}. {subj} â†’ {pred} ({count} æ¬¡)")
    
    # åˆ†æå…³ç³»-å®¢ä½“ç»„åˆ
    pred_obj_pairs = [(item['predicate'], item['object']) for item in data]
    pred_obj_counts = Counter(pred_obj_pairs)
    
    print("\nğŸ“Š æœ€å¸¸è§çš„å…³ç³»-å®¢ä½“ç»„åˆ:")
    for i, ((pred, obj), count) in enumerate(pred_obj_counts.most_common(10), 1):
        print(f"{i:2d}. {pred} â†’ {obj} ({count} æ¬¡)")
    
    return subj_pred_counts, pred_obj_counts

def create_network_analysis(data):
    """åˆ›å»ºç½‘ç»œåˆ†æ"""
    print("\n" + "="*50)
    print("ğŸ•¸ï¸ ç½‘ç»œç»“æ„åˆ†æ")
    print("="*50)
    
    # åˆ›å»ºNetworkXå›¾
    G = nx.Graph()
    
    for item in data:
        G.add_edge(item['subject'], item['object'], 
                  relation=item['predicate'])
    
    print(f"ç½‘ç»œèŠ‚ç‚¹æ•°: {G.number_of_nodes():,}")
    print(f"ç½‘ç»œè¾¹æ•°: {G.number_of_edges():,}")
    print(f"ç½‘ç»œå¯†åº¦: {nx.density(G):.6f}")
    
    # è®¡ç®—ä¸­å¿ƒæ€§æŒ‡æ ‡
    if G.number_of_nodes() > 0:
        degree_centrality = nx.degree_centrality(G)
        
        print("\nğŸ“Š æœ€é‡è¦çš„èŠ‚ç‚¹ï¼ˆæŒ‰åº¦ä¸­å¿ƒæ€§ï¼‰:")
        sorted_centrality = sorted(degree_centrality.items(), 
                                 key=lambda x: x[1], reverse=True)
        
        for i, (node, centrality) in enumerate(sorted_centrality[:10], 1):
            degree = G.degree(node)
            print(f"{i:2d}. {node:<30} (åº¦: {degree}, ä¸­å¿ƒæ€§: {centrality:.4f})")
    
    return G

def generate_summary_report(data, stats):
    """ç”Ÿæˆæ€»ç»“æŠ¥å‘Š"""
    print("\n" + "="*60)
    print("ğŸ“‹ çŸ¥è¯†å›¾è°±åˆ†ææ€»ç»“æŠ¥å‘Š")
    print("="*60)
    
    print(f"""
ğŸ¯ æ•°æ®è§„æ¨¡:
   â€¢ æ€»ä¸‰å…ƒç»„: {stats['total_triples']:,} ä¸ª
   â€¢ æ€»å®ä½“: {stats['total_entities']:,} ä¸ª
   â€¢ å…³ç³»ç±»å‹: {stats['unique_predicates']:,} ç§

ğŸ“Š å†…å®¹ç‰¹å¾:
   â€¢ è¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„æ”¿ç­–æ–‡æ¡£çŸ¥è¯†å›¾è°±
   â€¢ æ¶µç›–æ”¿åºœæ²»ç†ã€ç»æµå‘å±•ã€ç¤¾ä¼šæ°‘ç”Ÿç­‰å¤šä¸ªé¢†åŸŸ
   â€¢ å®ä½“å…³ç³»ä¸°å¯Œï¼Œä½“ç°äº†æ”¿ç­–æ–‡æ¡£çš„å¤æ‚æ€§

ğŸ” ä¸»è¦å‘ç°:
   â€¢ æ”¿åºœç›¸å…³å®ä½“æ˜¯å›¾è°±çš„æ ¸å¿ƒèŠ‚ç‚¹
   â€¢ å…³ç³»ç±»å‹å¤šæ ·ï¼Œåæ˜ äº†æ”¿ç­–çš„å¤šç»´åº¦ç‰¹å¾
   â€¢ æ–‡æ¡£ç»“æ„åŒ–ç¨‹åº¦é«˜ï¼Œä¿¡æ¯æå–æ•ˆæœè‰¯å¥½

ğŸ’¡ åº”ç”¨ä»·å€¼:
   â€¢ å¯ç”¨äºæ”¿ç­–åˆ†æå’Œå†³ç­–æ”¯æŒ
   â€¢ æœ‰åŠ©äºç†è§£æ”¿ç­–ä¹‹é—´çš„å…³è”å…³ç³»
   â€¢ ä¸ºæ”¿ç­–ç ”ç©¶æä¾›äº†ç»“æ„åŒ–çš„æ•°æ®åŸºç¡€
    """)

def main():
    """ä¸»å‡½æ•°"""
    file_path = "/Users/adrian/Documents/GitHub/What_kgllm/complete_policy_address_kg.json"
    
    try:
        # åŠ è½½æ•°æ®
        data = load_kg_data(file_path)
        
        # åŸºç¡€ç»Ÿè®¡
        stats = basic_statistics(data)
        
        # å„é¡¹åˆ†æ
        predicate_counts = analyze_predicates(data)
        entity_counts = analyze_entities(data)
        subject_counts, object_counts = analyze_subjects_objects(data)
        chunk_counts = analyze_chunks(data)
        topic_counts = find_key_topics(data)
        subj_pred_counts, pred_obj_counts = analyze_relationship_patterns(data)
        
        # ç½‘ç»œåˆ†æï¼ˆå¯¹äºå¤§å›¾å¯èƒ½è¾ƒæ…¢ï¼‰
        print("\nâš ï¸  ç½‘ç»œåˆ†æå¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´...")
        G = create_network_analysis(data)
        
        # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        generate_summary_report(data, stats)
        
        print(f"\nâœ… åˆ†æå®Œæˆï¼çŸ¥è¯†å›¾è°±åŒ…å«ä¸°å¯Œçš„æ”¿ç­–ä¿¡æ¯ã€‚")
        
    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()