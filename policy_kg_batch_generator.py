#!/usr/bin/env python3
"""
é¦™æ¸¯æ–½æ”¿æŠ¥å‘ŠçŸ¥è¯†å›¾è°±æ‰¹é‡ç”Ÿæˆå™¨ - æ”¹è¿›ç‰ˆ
æ”¯æŒåˆ†æ‰¹å¤„ç†ã€é”™è¯¯æ¢å¤å’Œè¿›åº¦ç›‘æ§
"""

import os
import json
import sys
import time
from datetime import datetime
from pathlib import Path
import traceback

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from src.knowledge_graph.main import process_text_in_chunks
from src.knowledge_graph.config import load_config

class PolicyKGBatchGenerator:
    """æ–½æ”¿æŠ¥å‘ŠçŸ¥è¯†å›¾è°±æ‰¹é‡ç”Ÿæˆå™¨ - æ”¹è¿›ç‰ˆ"""

    def __init__(self, data_dir="policy_data"):
        self.data_dir = Path(data_dir)
        self.setup_directories()
        self.batch_size = 5  # æ¯æ‰¹å¤„ç†5ä¸ªæ–‡ä»¶
        self.delay_between_files = 10  # æ–‡ä»¶é—´å»¶è¿Ÿ10ç§’
        self.delay_between_batches = 30  # æ‰¹æ¬¡é—´å»¶è¿Ÿ30ç§’

    def setup_directories(self):
        """è®¾ç½®ç›®å½•ç»“æ„"""
        directories = [
            self.data_dir / "raw_texts",      # åŸå§‹æ–‡æœ¬
            self.data_dir / "kg_json",        # çŸ¥è¯†å›¾è°±JSONæ•°æ®
            self.data_dir / "kg_html",        # çŸ¥è¯†å›¾è°±å¯è§†åŒ–
            self.data_dir / "logs",           # å¤„ç†æ—¥å¿—
            self.data_dir / "metadata"        # å…ƒæ•°æ®ä¿¡æ¯
        ]

        for dir_path in directories:
            dir_path.mkdir(parents=True, exist_ok=True)

    def create_policy_config(self):
        """åˆ›å»ºé’ˆå¯¹æ–½æ”¿æŠ¥å‘Šçš„ä¸“ç”¨é…ç½®"""
        base_config = load_config()

        # é’ˆå¯¹æ–½æ”¿æŠ¥å‘Šä¼˜åŒ–çš„é…ç½®
        policy_config = base_config.copy()
        policy_config.update({
            'chunking': {
                'chunk_size': 120,  # é€‚ä¸­çš„å—å¤§å°
                'overlap': 25       # é€‚å½“çš„é‡å 
            },
            'standardization': {
                'enabled': True,
                'use_llm_for_entities': True,
                'focus_entities': [
                    'è¡Œæ”¿é•¿å®˜', 'ç‰¹åŒºæ”¿åºœ', 'ä¸­å¤®æ”¿åºœ', 'ç«‹æ³•ä¼š', 'è¡Œæ”¿ä¼šè®®',
                    'ä¸€å›½ä¸¤åˆ¶', 'åŸºæœ¬æ³•', 'å›½å®¶å®‰å…¨', 'é¦™æ¸¯å›½å®‰æ³•',
                    'ç»æµå‘å±•', 'æ°‘ç”Ÿæ”¹å–„', 'æ•™è‚²æ”¿ç­–', 'æˆ¿å±‹æ”¿ç­–', 'åŒ»ç–—æ”¿ç­–',
                    'å¤§æ¹¾åŒº', 'ç²¤æ¸¯æ¾³å¤§æ¹¾åŒº', 'åˆ›æ–°ç§‘æŠ€', 'é’å¹´å‘å±•',
                    'å›½é™…é‡‘èä¸­å¿ƒ', 'è´¸æ˜“ä¸­å¿ƒ', 'èˆªè¿ä¸­å¿ƒ'
                ]
            },
            'inference': {
                'enabled': True,
                'use_llm_for_inference': True,
                'apply_transitive': True
            }
        })

        return policy_config

    def get_available_files(self):
        """è·å–å¯ç”¨çš„æ–‡æœ¬æ–‡ä»¶åˆ—è¡¨"""
        available_files = []
        years = range(1997, 2025)

        for year in years:
            text_file = self.data_dir / "raw_texts" / f"policy_address_{year}.txt"
            if text_file.exists():
                available_files.append((year, text_file))

        return sorted(available_files)

    def get_completed_files(self):
        """è·å–å·²å®Œæˆçš„æ–‡ä»¶åˆ—è¡¨"""
        completed_years = set()
        kg_dir = self.data_dir / "kg_json"

        for kg_file in kg_dir.glob("policy_kg_*.json"):
            try:
                year = int(kg_file.stem.split('_')[-1])
                completed_years.add(year)
            except ValueError:
                continue

        return completed_years

    def generate_single_kg(self, year, text_content, config):
        """ä¸ºå•å¹´æ–½æ”¿æŠ¥å‘Šç”ŸæˆçŸ¥è¯†å›¾è°±"""
        print(f"\nğŸ“„ å¤„ç† {year} å¹´æ–½æ”¿æŠ¥å‘Š...")
        print(f"   æ–‡æœ¬é•¿åº¦: {len(text_content):,} å­—ç¬¦")

        try:
            start_time = time.time()

            # å¤„ç†æ–‡æœ¬ç”ŸæˆçŸ¥è¯†å›¾è°±
            kg_data = process_text_in_chunks(config, text_content, debug=False)

            if not kg_data:
                print(f"âŒ {year}å¹´çŸ¥è¯†å›¾è°±ç”Ÿæˆå¤±è´¥ - æ— æ•°æ®è¿”å›")
                return None

            processing_time = time.time() - start_time

            # æ·»åŠ å…ƒæ•°æ®
            metadata = {
                'year': year,
                'generated_at': datetime.now().isoformat(),
                'processing_time_seconds': round(processing_time, 2),
                'total_triples': len(kg_data),
                'unique_entities': len(self._get_unique_entities(kg_data)),
                'unique_relations': len(set(item['predicate'] for item in kg_data)),
                'text_length': len(text_content),
                'chunks_processed': max([item.get('chunk', 1) for item in kg_data]) if kg_data else 0
            }

            # ä¿å­˜JSONæ•°æ®
            json_file = self.data_dir / "kg_json" / f"policy_kg_{year}.json"
            output_data = {
                'metadata': metadata,
                'knowledge_graph': kg_data
            }

            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, ensure_ascii=False, indent=2)

            print(f"âœ… {year}å¹´å¤„ç†å®Œæˆ (è€—æ—¶: {processing_time:.1f}ç§’):")
            print(f"   â€¢ ä¸‰å…ƒç»„æ•°é‡: {metadata['total_triples']}")
            print(f"   â€¢ å”¯ä¸€å®ä½“: {metadata['unique_entities']}")
            print(f"   â€¢ å…³ç³»ç±»å‹: {metadata['unique_relations']}")
            print(f"   â€¢ å¤„ç†å—æ•°: {metadata['chunks_processed']}")

            return metadata

        except Exception as e:
            print(f"âŒ {year}å¹´å¤„ç†å‡ºé”™: {str(e)}")
            print(f"   é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")

            # ä¿å­˜é”™è¯¯æ—¥å¿—
            error_log = {
                'year': year,
                'error_time': datetime.now().isoformat(),
                'error_message': str(e),
                'error_traceback': traceback.format_exc()
            }

            error_file = self.data_dir / "logs" / f"error_{year}.json"
            with open(error_file, 'w', encoding='utf-8') as f:
                json.dump(error_log, f, ensure_ascii=False, indent=2)

            return None

    def _get_unique_entities(self, kg_data):
        """è·å–å”¯ä¸€å®ä½“é›†åˆ"""
        entities = set()
        for item in kg_data:
            entities.add(item['subject'])
            entities.add(item['object'])
        return entities

    def batch_generate(self, start_year=None, end_year=None, force_regenerate=False):
        """æ‰¹é‡ç”ŸæˆçŸ¥è¯†å›¾è°±"""
        print("ğŸš€ å¼€å§‹æ‰¹é‡ç”Ÿæˆé¦™æ¸¯æ–½æ”¿æŠ¥å‘ŠçŸ¥è¯†å›¾è°±")
        print("=" * 60)

        # è·å–é…ç½®
        config = self.create_policy_config()

        # è·å–å¯ç”¨æ–‡ä»¶
        available_files = self.get_available_files()
        if not available_files:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•æ–½æ”¿æŠ¥å‘Šæ–‡æœ¬æ–‡ä»¶")
            return {}

        # è¿‡æ»¤å¹´ä»½èŒƒå›´
        if start_year or end_year:
            available_files = [
                (year, file) for year, file in available_files
                if (not start_year or year >= start_year) and (not end_year or year <= end_year)
            ]

        # è·å–å·²å®Œæˆçš„æ–‡ä»¶
        completed_years = self.get_completed_files()

        # è¿‡æ»¤æœªå®Œæˆçš„æ–‡ä»¶
        if not force_regenerate:
            pending_files = [(year, file) for year, file in available_files if year not in completed_years]
            if completed_years:
                print(f"ğŸ“‹ å·²å®Œæˆ: {sorted(completed_years)}")
        else:
            pending_files = available_files
            print("ğŸ”„ å¼ºåˆ¶é‡æ–°ç”Ÿæˆæ‰€æœ‰æ–‡ä»¶")

        if not pending_files:
            print("âœ… æ‰€æœ‰æ–‡ä»¶éƒ½å·²å¤„ç†å®Œæˆ")
            return self.check_data_status()

        print(f"ğŸ“Š å¾…å¤„ç†æ–‡ä»¶: {len(pending_files)} ä¸ª")
        print(f"ğŸ“ å¹´ä»½åˆ—è¡¨: {[year for year, _ in pending_files]}")

        # åˆ†æ‰¹å¤„ç†
        results = {}
        total_batches = (len(pending_files) + self.batch_size - 1) // self.batch_size

        for batch_idx in range(total_batches):
            start_idx = batch_idx * self.batch_size
            end_idx = min(start_idx + self.batch_size, len(pending_files))
            batch_files = pending_files[start_idx:end_idx]

            print(f"\nğŸ”„ å¤„ç†æ‰¹æ¬¡ {batch_idx + 1}/{total_batches}")
            print(f"   æ–‡ä»¶: {[year for year, _ in batch_files]}")

            # å¤„ç†æ‰¹æ¬¡ä¸­çš„æ¯ä¸ªæ–‡ä»¶
            for i, (year, text_file) in enumerate(batch_files):
                try:
                    # è¯»å–æ–‡æœ¬
                    with open(text_file, 'r', encoding='utf-8') as f:
                        text_content = f.read()

                    # ç”ŸæˆçŸ¥è¯†å›¾è°±
                    metadata = self.generate_single_kg(year, text_content, config)
                    if metadata:
                        results[year] = metadata

                    # æ–‡ä»¶é—´å»¶è¿Ÿ
                    if i < len(batch_files) - 1:
                        print(f"â³ ç­‰å¾… {self.delay_between_files} ç§’...")
                        time.sleep(self.delay_between_files)

                except Exception as e:
                    print(f"âŒ è¯»å–{year}å¹´æ–‡ä»¶å¤±è´¥: {str(e)}")
                    continue

            # æ‰¹æ¬¡é—´å»¶è¿Ÿ
            if batch_idx < total_batches - 1:
                print(f"\nâ¸ï¸  æ‰¹æ¬¡å®Œæˆï¼Œç­‰å¾… {self.delay_between_batches} ç§’åç»§ç»­...")
                time.sleep(self.delay_between_batches)

        # ä¿å­˜æ‰¹é‡å¤„ç†ç»“æœ
        self._save_batch_results(results)

        print(f"\nğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆ!")
        print(f"âœ… æœ¬æ¬¡æˆåŠŸå¤„ç†: {len(results)} ä¸ªå¹´ä»½")
        print(f"ğŸ“ æ•°æ®ä¿å­˜åœ¨: {self.data_dir}/kg_json/")

        return results

    def _save_batch_results(self, results):
        """ä¿å­˜æ‰¹é‡å¤„ç†ç»“æœæ‘˜è¦"""
        if not results:
            return

        summary = {
            'generated_at': datetime.now().isoformat(),
            'batch_size': len(results),
            'years_processed': sorted(results.keys()),
            'summary_stats': {
                'total_triples': sum(r['total_triples'] for r in results.values()),
                'avg_triples_per_year': sum(r['total_triples'] for r in results.values()) / len(results),
                'total_entities': sum(r['unique_entities'] for r in results.values()),
                'avg_entities_per_year': sum(r['unique_entities'] for r in results.values()) / len(results),
                'total_processing_time': sum(r.get('processing_time_seconds', 0) for r in results.values())
            },
            'yearly_details': results
        }

        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        summary_file = self.data_dir / "metadata" / f"batch_generation_{timestamp}.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)

        print(f"ğŸ“‹ æ‰¹é‡å¤„ç†æ‘˜è¦å·²ä¿å­˜: {summary_file}")

    def check_data_status(self):
        """æ£€æŸ¥æ•°æ®ç”ŸæˆçŠ¶æ€"""
        print("ğŸ” æ£€æŸ¥çŸ¥è¯†å›¾è°±æ•°æ®çŠ¶æ€...")
        print("=" * 50)

        kg_dir = self.data_dir / "kg_json"
        existing_files = list(kg_dir.glob("policy_kg_*.json"))

        if not existing_files:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•çŸ¥è¯†å›¾è°±æ•°æ®æ–‡ä»¶")
            return {}

        data_status = {}
        total_triples = 0
        total_entities = 0

        for file_path in sorted(existing_files):
            try:
                year = int(file_path.stem.split('_')[-1])
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                metadata = data.get('metadata', {})
                triples = metadata.get('total_triples', 0)
                entities = metadata.get('unique_entities', 0)
                relations = metadata.get('unique_relations', 0)

                data_status[year] = {
                    'file': file_path,
                    'triples': triples,
                    'entities': entities,
                    'relations': relations,
                    'generated_at': metadata.get('generated_at', 'Unknown')
                }

                total_triples += triples
                total_entities += entities

                print(f"âœ… {year}å¹´: {triples:,} ä¸‰å…ƒç»„, {entities:,} å®ä½“, {relations} å…³ç³»")

            except Exception as e:
                print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {str(e)}")

        print(f"\nğŸ“Š æ€»è®¡: {len(data_status)} ä¸ªå¹´ä»½")
        print(f"ğŸ“ˆ æ€»ä¸‰å…ƒç»„: {total_triples:,}")
        print(f"ğŸ·ï¸  æ€»å®ä½“: {total_entities:,}")

        return data_status

def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description='é¦™æ¸¯æ–½æ”¿æŠ¥å‘ŠçŸ¥è¯†å›¾è°±æ‰¹é‡ç”Ÿæˆå™¨ - æ”¹è¿›ç‰ˆ')
    parser.add_argument('--generate', action='store_true', help='æ‰¹é‡ç”ŸæˆçŸ¥è¯†å›¾è°±')
    parser.add_argument('--check', action='store_true', help='æ£€æŸ¥æ•°æ®çŠ¶æ€')
    parser.add_argument('--start-year', type=int, help='å¼€å§‹å¹´ä»½')
    parser.add_argument('--end-year', type=int, help='ç»“æŸå¹´ä»½')
    parser.add_argument('--force', action='store_true', help='å¼ºåˆ¶é‡æ–°ç”Ÿæˆ')
    parser.add_argument('--data-dir', default='policy_data', help='æ•°æ®ç›®å½•è·¯å¾„')

    args = parser.parse_args()

    generator = PolicyKGBatchGenerator(args.data_dir)

    if args.generate:
        generator.batch_generate(
            start_year=args.start_year,
            end_year=args.end_year,
            force_regenerate=args.force
        )
    elif args.check:
        generator.check_data_status()
    else:
        print("è¯·æŒ‡å®šæ“ä½œ:")
        print("  --generate           æ‰¹é‡ç”ŸæˆçŸ¥è¯†å›¾è°±")
        print("  --check              æ£€æŸ¥æ•°æ®çŠ¶æ€")
        print("  --start-year YYYY    æŒ‡å®šå¼€å§‹å¹´ä»½")
        print("  --end-year YYYY      æŒ‡å®šç»“æŸå¹´ä»½")
        print("  --force              å¼ºåˆ¶é‡æ–°ç”Ÿæˆ")

if __name__ == "__main__":
    main()
